services:
  react-app:
    image: client-react-nginx:prod
    build:
      context: ./react-app
      dockerfile: Dockerfile
      args:
       - VITE_APP_BACKEND_ADDRESS=/api/generate
    init: true
    restart: unless-stopped
    networks:
      - frontend
    ports:
      - 80:80
  api-server:
    image: api-server:prod
    build:
      context: ./api_server
      dockerfile: Dockerfile
    init: true
    env_file: ".env" 
    networks:
      - frontend
      - backend
    ports:
      - 8080:8080
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  db:
    image: pgvector/pgvector:pg17
    restart: unless-stopped
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres/vector_init.sql:/docker-entrypoint-initdb.d/vector_init.sql:ro
    environment:
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_DB=${POSTGRES_DB}
    networks:
      - backend
    ports:
      - 5432:5432
  vllm-server:
    image: vllm/vllm-openai:v0.7.0
    runtime: nvidia
    volumes:
      - ./.cache/huggingface:/root/.cache/huggingface
    environment:
      HUGGING_FACE_HUB_TOKEN: "${HF_TOKEN}"
    ipc: host
    # --model meta-llama/Llama-3.2-3B-Instruct
    command: >
      --model ${VLLM_MODEL}
      --max-model-len 30000
      --enable-auto-tool-choice
      --tool-call-parser llama3_json
    networks:
      - backend
    ports:
      - 8000:8000
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
networks:
  frontend:
  backend:
volumes:
  pgdata: